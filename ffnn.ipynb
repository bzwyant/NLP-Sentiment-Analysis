{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import collections\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Importing utility functions from Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, InputLayer\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    97764\n",
       " 0    80800\n",
       "-1    52568\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenized_df.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "\n",
    "data[\"label\"] = data[\"label\"].astype('int')\n",
    "data[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Frequency'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[\"tokens\"].apply(lambda x: len(x)).plot(kind='hist', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import keyedvectors,KeyedVectors\n",
    "\n",
    "embeddings = KeyedVectors.load_word2vec_format('embeddings_unks_25.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time_ns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def read_embeddings(embeddings: KeyedVectors,tokenizer: Tokenizer) -> tuple[dict[str,list], dict[int,list]]:\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters and return values are up to you.\n",
    "    '''\n",
    "    embeddings_size = len(embeddings[embeddings.index_to_key[0]])\n",
    "    # str : array mapping\n",
    "    word_embeddings = {w:embeddings[w] for w in tokenizer.word_index } # TODO: mmake compatible with pre-made embeddings\n",
    "    # int : array mapping\n",
    "    encoded_embeddings = {tokenizer.word_index[w]:word_embeddings[w] for w in word_embeddings}\n",
    "    encoded_embeddings[0] = np.zeros(embeddings_size)\n",
    "    return word_embeddings, encoded_embeddings\n",
    "\n",
    "def get_X_and_y(data : pd.DataFrame, tokenizer: Tokenizer, enc_embed:dict):\n",
    "    '''Returns X and y from the data.\n",
    "    '''\n",
    "    # convert text to sequences\n",
    "\n",
    "    # pad sequences to length 50\n",
    "    padded_seqs = pad_sequences(tokenizer.texts_to_sequences(data[\"tokens\"]), maxlen=50, padding='post')\n",
    "    # X =  np.array([[weight for token in seq for weight in enc_embed[token]] for seq in padded_seqs])\n",
    "    y = to_categorical(data[\"label\"].values+1)\n",
    "    return padded_seqs,y\n",
    "\n",
    "def get_bow_and_labels(data : pd.DataFrame, tokenizer: Tokenizer):\n",
    "    '''Returns X and y from the data.\n",
    "    '''\n",
    "    # convert text to sequences\n",
    "    # feature_vocab = [word for word, count in counter.most_common(100)]\n",
    "\n",
    "    X = tokenizer.texts_to_matrix(data[\"tokens\"], mode='count')\n",
    "    y = to_categorical(data[\"label\"].values+1)\n",
    "    return X,y\n",
    "\n",
    "# def data_generator(X, y, batch_size: int, tokenizer: Tokenizer, enc_embed:dict) -> tuple[list,list]:\n",
    "#     '''\n",
    "#     Returns data generator to be used by feed_forward\n",
    "#     https://wiki.python.org/moin/Generators\n",
    "#     https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "#     Yields batches of embeddings and labels to go with them.\n",
    "#     Use one hot vectors to encode the labels \n",
    "#     (see the to_categorical function)\n",
    "    \n",
    "#     '''\n",
    "#     i = 0\n",
    "#     for i in range(0,len(y),batch_size):\n",
    "#         next_i = min(len(y), i+batch_size)\n",
    "#         # for each sequence in the batch, flatten all word embedding vectors into one vector\n",
    "#         embeddings = np.array([np.array([weight for word_index in sequence for weight in enc_embed[word_index]]) for sequence in X[i:next_i]])\n",
    "#         # generate categorical data\n",
    "#         labels = to_categorical(y[i:next_i]-1,num_classes=len(tokenizer.word_counts))\n",
    "#         yield embeddings, labels\n",
    "\n",
    "\n",
    "def data_generator(padded_seqs, y, batch_size: int, enc_embed) -> tuple[list,list]:\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    while True:\n",
    "        i = 0\n",
    "        for i in range(0,len(y),batch_size):\n",
    "            next_i = min(len(y), i+batch_size)\n",
    "            # for each sequence in the batch, flatten all word embedding vectors into one vector\n",
    "            X_batch = padded_seqs[i:next_i]\n",
    "            embeddings =  np.array([[weight for token in seq for weight in enc_embed[token]] for seq in X_batch])\n",
    "\n",
    "            # generate categorical data\n",
    "            labels = y[i:next_i]\n",
    "            yield embeddings, labels\n",
    "\n",
    "def bow_generator(X, y, batch_size: int, enc_embed) -> tuple[list,list]:\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    '''\n",
    "    while True:\n",
    "        i = 0\n",
    "        for i in range(0,len(y),batch_size):\n",
    "            next_i = min(len(y), i+batch_size)\n",
    "            # for each sequence in the batch, flatten all word embedding vectors into one vector\n",
    "            X_batch = X[i:next_i]\n",
    "            embeddings =  np.array([[weight for token in bows for weight in enc_embed[token]] for bows in X_batch])\n",
    "\n",
    "            # generate categorical data\n",
    "            labels = y[i:next_i]\n",
    "            yield embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56994\n",
      "(23113, 100)\n",
      "[0. 5. 2. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "## SPLIT DATA ##\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"UNK\")\n",
    "tokenizer.fit_on_texts(data[\"tokens\"])\n",
    "word_counter = Counter([token for line in data[\"tokens\"] for token in line])\n",
    "print(len(word_counter))\n",
    "word_counter.most_common(100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# select 10% of rows from data dataframe\n",
    "small_data = data.sample(frac=0.10, random_state=69)\n",
    "\n",
    "\n",
    "word_embed, enc_embed = read_embeddings(embeddings, tokenizer)\n",
    "# X,y = get_X_and_y(small_data,tokenizer, enc_embed)\n",
    "# X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "\n",
    "# print(X_test)\n",
    "# print(X_test.shape)\n",
    "# print(len(X_test))\n",
    "\n",
    "X,y = get_bow_and_labels(small_data, tokenizer)\n",
    "print(X.shape)\n",
    "print(X[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given lines of text (arraylike) and embeddings (model.wv)\n",
    "def train_model(X, y,embeddings:KeyedVectors, tokenizer: Tokenizer, enc_embed:dict,epochs=100, x_val=None, y_val=None):\n",
    "   \n",
    "\n",
    "    # print(\"X:\",X)\n",
    "    # print(\"y:\",y)\n",
    "\n",
    "    # get word embeddings dictionaries for our corpus\n",
    "    \n",
    "\n",
    "    embeddings_size = len(enc_embed[1])\n",
    "    batch_size = 32\n",
    "    data_gen = data_generator(X, y, batch_size=batch_size, enc_embed=enc_embed)\n",
    "    data_gen_val = data_generator(x_val, y_val, batch_size=batch_size, enc_embed=enc_embed)\n",
    "    sample = next(data_gen)\n",
    "    # print(sample.shape)\n",
    "\n",
    "    # Define the model architecture using Keras Sequential API\n",
    "    model = Sequential()\n",
    "    model.add(layer_h0 := Dense(500, input_shape=(len(sample[0][0]),))) \n",
    "    model.add(layer_h1 := Dense(250, activation='relu'))\n",
    "    model.add(layer_h1 := Dense(100, activation='relu'))\n",
    "    model.add(layer_h1 := Dense(50, activation='relu'))\n",
    "    model.add(layer_o := Dense(len(y[0]),activation=\"softmax\")) \n",
    "\n",
    "    # setup model for learning\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(data_gen, epochs=epochs, steps_per_epoch=len(X)//32, verbose=1, \n",
    "                        validation_steps=len(x_val)//32, validation_data=data_gen_val)\n",
    "\n",
    "    # return the model, tokenizer, and embedding dict for sentence generation use later\n",
    "    return model,tokenizer,history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "722/722 [==============================] - 215s 297ms/step - loss: 0.7178 - accuracy: 0.7071 - val_loss: 0.5458 - val_accuracy: 0.7952\n",
      "Epoch 2/10\n",
      "722/722 [==============================] - 219s 303ms/step - loss: 0.5578 - accuracy: 0.7840 - val_loss: 0.4923 - val_accuracy: 0.8096\n",
      "Epoch 3/10\n",
      "722/722 [==============================] - 219s 303ms/step - loss: 0.4921 - accuracy: 0.8013 - val_loss: 0.4886 - val_accuracy: 0.8086\n",
      "Epoch 4/10\n",
      "722/722 [==============================] - 212s 293ms/step - loss: 0.4381 - accuracy: 0.8214 - val_loss: 0.4545 - val_accuracy: 0.8205\n",
      "Epoch 5/10\n",
      "722/722 [==============================] - 203s 281ms/step - loss: 0.3975 - accuracy: 0.8378 - val_loss: 0.4126 - val_accuracy: 0.8365\n",
      "Epoch 6/10\n",
      "722/722 [==============================] - 204s 283ms/step - loss: 0.3483 - accuracy: 0.8596 - val_loss: 0.3722 - val_accuracy: 0.8512\n",
      "Epoch 7/10\n",
      "722/722 [==============================] - 205s 284ms/step - loss: 0.3003 - accuracy: 0.8770 - val_loss: 0.3604 - val_accuracy: 0.8583\n",
      "Epoch 8/10\n",
      "722/722 [==============================] - 205s 285ms/step - loss: 0.2663 - accuracy: 0.8950 - val_loss: 0.3222 - val_accuracy: 0.8779\n",
      "Epoch 9/10\n",
      "722/722 [==============================] - 208s 288ms/step - loss: 0.2431 - accuracy: 0.9061 - val_loss: 0.3025 - val_accuracy: 0.8924\n",
      "Epoch 10/10\n",
      "722/722 [==============================] - 207s 286ms/step - loss: 0.2064 - accuracy: 0.9188 - val_loss: 0.2678 - val_accuracy: 0.9039\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-16 20:21:45    151905040\n",
      "metadata.json                                  2023-04-16 20:21:45           64\n",
      "config.json                                    2023-04-16 20:21:45         2704\n",
      "722/722 [==============================] - 81s 113ms/step - loss: 1.3123 - accuracy: 0.7342\n",
      "[1.3122572898864746, 0.7342018485069275]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"UNK\")\n",
    "tokenizer.fit_on_texts(data[\"tokens\"])\n",
    "# word_counter = Counter([token for line in data[\"tokens\"] for token in line])\n",
    "# print(len(word_counter))\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "small_data = data.sample(frac=0.2, random_state=69)\n",
    "\n",
    "for embbeddings_size in [25]: #,50,100,200]:\n",
    "    if os.path.exists(f'ffnn{embbeddings_size}_{epochs}_epochs.pickle'):\n",
    "        print(\"ffnn already trained for this embedding size\")\n",
    "        continue\n",
    "    embeddings = KeyedVectors.load_word2vec_format(f'embeddings_unks_{embbeddings_size}.txt', binary=False)\n",
    "    word_embed, enc_embed = read_embeddings(embeddings, tokenizer)\n",
    "    X,y = get_bow_and_labels(small_data,tokenizer)\n",
    "    X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X, y, test_size=0.5, random_state=69)\n",
    "    net, tok, history = train_model(X_train, y_train,embeddings,tokenizer, enc_embed, epochs=epochs, x_val=X_val, y_val=y_val)\n",
    "    with open(f'ffnn{embbeddings_size}.pickle', 'wb') as handle:\n",
    "        pickle.dump(net, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(net.evaluate(data_generator(X_test, y_test, batch_size=32, enc_embed=enc_embed), steps=len(X_test)//32))\n",
    "    \n",
    "\n",
    "# word_embed, enc_embed = read_embeddings(embeddings, tokenizer)\n",
    "# X,y = get_X_and_y(small_data,tokenizer, enc_embed)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "\n",
    "\n",
    "# net2, tok2 = train_model(X_train, y_train,embeddings,tokenizer, enc_embed, epochs=100)\n",
    "# with open('ffnn200.pickle', 'wb') as handle:\n",
    "#     pickle.dump(net2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "variables.h5                                   2023-04-16 15:34:12     22705992\n",
      "metadata.json                                  2023-04-16 15:34:12           64\n",
      "config.json                                    2023-04-16 15:34:12         2701\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_3\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_4\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "144/144 [==============================] - 4s 26ms/step\n",
      "[[0.28692693 0.45171463 0.2613585 ]\n",
      " [0.12987779 0.7477042  0.12241785]\n",
      " [0.33389708 0.2708141  0.39528883]\n",
      " ...\n",
      " [0.23422243 0.0379648  0.72781277]\n",
      " [0.4638666  0.15858041 0.37755305]\n",
      " [0.03853124 0.5302264  0.43124238]]\n",
      "model hasnt been trained yet\n",
      "model hasnt been trained yet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for embbeddings_size in [25,50,100]:#,200]:\n",
    "    if not os.path.exists(f'ffnn{embbeddings_size}.pickle'):\n",
    "        print(\"model hasnt been trained yet\")\n",
    "        continue\n",
    "    embeddings = KeyedVectors.load_word2vec_format(f'embeddings_unks_{embbeddings_size}.txt', binary=False)\n",
    "    word_embed, enc_embed = read_embeddings(embeddings, tokenizer)\n",
    "    X,y = get_X_and_y(small_data,tokenizer, enc_embed)\n",
    "    X_train, X_test, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=69)\n",
    "    # load net from pickle file \n",
    "    print(embbeddings_size)\n",
    "    with open(f'ffnn{embbeddings_size}.pickle', 'rb') as handle:\n",
    "        net : Sequential = pickle.load(handle)\n",
    "\n",
    "    print(net.predict(data_generator(X_test, y_val, batch_size=32, enc_embed=enc_embed), steps=len(X_test)//32))\n",
    "    # print(net.evaluate(data_generator(X_test, y_test, batch_size=32, enc_embed=enc_embed), steps=len(X_test)//32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
