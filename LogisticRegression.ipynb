{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a893671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double precision (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    true_pos_count = 0\n",
    "    true_false_pos_count = 0\n",
    "\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1':\n",
    "            if predicted_labels[i] == '1':\n",
    "                true_pos_count += 1\n",
    "                true_false_pos_count += 1\n",
    "        else:\n",
    "            if predicted_labels[i] == '1':\n",
    "                true_false_pos_count += 1\n",
    "\n",
    "    return true_pos_count / true_false_pos_count\n",
    "\n",
    "\n",
    "def recall(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the recall for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double recall (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    true_pos_count = 0\n",
    "    true_pos_false_neg_count = 0\n",
    "\n",
    "    for i in range(len(gold_labels)):\n",
    "        if gold_labels[i] == '1':\n",
    "            if predicted_labels[i] == '1':\n",
    "                true_pos_count += 1\n",
    "            true_pos_false_neg_count += 1\n",
    "\n",
    "    return true_pos_count / true_pos_false_neg_count\n",
    "\n",
    "\n",
    "def f1(gold_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the f1 for a set of predicted labels give the gold (ground truth) labels.\n",
    "    Parameters:\n",
    "        gold_labels (list): a list of labels assigned by hand (\"truth\")\n",
    "        predicted_labels (list): a corresponding list of labels predicted by the system\n",
    "    Returns: double f1 (a number from 0 to 1)\n",
    "    \"\"\"\n",
    "    p = precision(gold_labels, predicted_labels)\n",
    "    r = recall(gold_labels, predicted_labels)\n",
    "\n",
    "    denominator = p + r\n",
    "    numerator = 2 * p * r\n",
    "\n",
    "    if denominator == 0 or numerator == 0:\n",
    "        return 0\n",
    "\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_from_tuple(tuples, string):\n",
    "    \"\"\"\n",
    "    given a list of tuples, find the tuple whose first element matches the string\n",
    "    Args:\n",
    "        tuples: a list of tuples [(string, value)...]\n",
    "        string: the string matcher\n",
    "\n",
    "    Returns: a tuple if found, an empty list if not\n",
    "\n",
    "    \"\"\"\n",
    "    for t in tuples:\n",
    "        if t[0] == string:\n",
    "            return t\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def read_text(filepath, dictionary=None):\n",
    "    \"\"\"\n",
    "    gets the text and add it to the dictionary\n",
    "    Args:\n",
    "        filepath: the filepath of the text file\n",
    "        dictionary: the dictionary storage for True to get quick lookup\n",
    "\n",
    "    Returns: the dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    if dictionary is None:\n",
    "        dictionary = {}\n",
    "\n",
    "    f = open(filepath, \"r\", encoding='utf8')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line not in dictionary.keys():\n",
    "            dictionary[line] = True\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def get_labels(examples):\n",
    "    \"\"\"\n",
    "    Returns a list of the labels from the examples\n",
    "    Args:\n",
    "        examples: a list of tuples [(id, text, label)...]\n",
    "\n",
    "    Returns: list of labels [label, label, ...]\n",
    "    \"\"\"\n",
    "    final_list = []\n",
    "    for example in examples:\n",
    "        final_list.append(example[2])\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n",
    "def lost_function_for_each_weight(sigmoid, result, weight):\n",
    "    \"\"\"\n",
    "    Returns the lost value for each weight\n",
    "    Args:\n",
    "        sigmoid: the sigmoid value\n",
    "        weight: our current weight value\n",
    "        result: whether it was 1 or 0\n",
    "    Returns: the lost value\n",
    "    \"\"\"\n",
    "\n",
    "    difference = sigmoid - result\n",
    "\n",
    "    return difference * weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7c57694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, keyedvectors\n",
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# CSV FILE SOURCE FOR BAD WORDS\n",
    "# https://github.com/surge-ai/profanity/blob/main/profanity_en.csv\n",
    "\n",
    "class LogisticRegressionModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Features - Count of Positive Lexicon,\n",
    "        #            Count of Negative Lexicon,\n",
    "        #            If exclamation mark \"!\" is in the doc (1 if in, 0 otherwise)\n",
    "        #            Difference of positive lexicon and negative lexicon\n",
    "        #            If the words \"another\", \"typical\" is in the doc (1 if in, 0 otherwise)\n",
    "\n",
    "        self.positive_words = read_text(\"positive-words.txt\")\n",
    "        self.negative_words = read_text(\"negative-words.txt\")\n",
    "        \n",
    "        self.profanity = list(pd.read_csv('profanity_en.csv')['text'])\n",
    "\n",
    "        # our features weights\n",
    "        self.weights = {}\n",
    "        # Learning step\n",
    "        self.step = 0.1\n",
    "        # bias\n",
    "        self.bias = 0.1\n",
    "        \n",
    "        self.word_embeddings = None\n",
    "        \n",
    "        self.weights['profanity'] = (random.random() * 3 + 1) * random.choice([1, -1])\n",
    "        self.weights['difference'] = (random.random() * 3 + 1) * random.choice([1, -1])\n",
    "        self.weights['bias'] = self.bias\n",
    "        self.weights['difference embeddings'] = (random.random() * 3 + 1) * random.choice([1, -1])\n",
    "        \n",
    "        \n",
    "       \n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        Trains the classifier based on the given examples\n",
    "        Parameters:\n",
    "          examples - a list of tuples of strings formatted [(id, example_text, label), (id, example_text, label)....]\n",
    "        Return: None\n",
    "        \"\"\"\n",
    "        if os.path.isfile('word2vec.txt'):\n",
    "            self.word_embeddings = keyedvectors.KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "        else:\n",
    "            self.word_embeddings = Word2Vec(examples, sg=1, window=5, vector_size=200, min_count=1)\n",
    "            self.word_embeddings.wv.save_word2vec_format('word2vec.txt', binary=False)\n",
    "        \n",
    "        # We will be doing Stochastic Gradient Descent on each individual example\n",
    "        for example in examples:\n",
    "            # Get the features of each example\n",
    "            features = self.featurize(example[0])\n",
    "            label = example[1]\n",
    "\n",
    "            self.gradient_descent(features, label, iterations=1000)\n",
    "\n",
    "    def gradient_descent(self, features, label, iterations=20):\n",
    "        \"\"\"\n",
    "        Performs the gradient descent on each feature\n",
    "        Args:\n",
    "            features: the features that our data has\n",
    "            label: \"1\" or \"0\"\n",
    "            iterations: the number of iterations we will run, default 20\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(iterations):\n",
    "            sigmoid_result = self.sigmoid(features)\n",
    "            y = int(label)\n",
    "\n",
    "            # Calculating loss for each weight:\n",
    "\n",
    "            for weight in self.weights.keys():\n",
    "                loss_value = lost_function_for_each_weight(sigmoid_result, y, get_from_tuple(features, weight)[1])\n",
    "                change = self.step * loss_value\n",
    "\n",
    "                self.weights[weight] = self.weights[weight] - change\n",
    "\n",
    "    def sigmoid(self, features):\n",
    "        \"\"\"\n",
    "        Gets the sigmoid, or P(y=1 | x)\n",
    "        Args:\n",
    "            features: the list of tuples that corresponds to the features\n",
    "\n",
    "        Returns: the sigmoid\n",
    "\n",
    "        \"\"\"\n",
    "        dot_product = 0\n",
    "\n",
    "        for i in self.weights.keys():\n",
    "            dot_product += self.weights[i] * get_from_tuple(features, i)[1]\n",
    "            \n",
    "        z = dot_product + self.weights['bias']\n",
    "        \n",
    "        print(z)\n",
    "        \n",
    "        denominator = 1 + np.power(np.e, -1 * z)\n",
    "\n",
    "        return 1 / denominator\n",
    "\n",
    "    def score(self, data):\n",
    "        \"\"\"\n",
    "        Score a given piece of text\n",
    "        youâ€™ll compute e ^ (log(p(c)) + sum(log(p(w_i | c))) here\n",
    "\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: dict of class: score mappings\n",
    "        return a dictionary of the values of P(data | c)  for each class,\n",
    "        as in section 4.3 of the textbook e.g. {\"0\": 0.000061, \"1\": 0.000032}\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.featurize(data)\n",
    "        sigmoid = self.sigmoid(features)\n",
    "\n",
    "        return {\"1\": sigmoid, \"0\": 1 - sigmoid}\n",
    "\n",
    "    def classify(self, data):\n",
    "        \"\"\"\n",
    "        Label a given piece of text\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: string class label\n",
    "        \"\"\"\n",
    "        scores = self.score(data)\n",
    "\n",
    "        max_score = 0\n",
    "        classified = \"\"\n",
    "\n",
    "        for each_class in scores.keys():\n",
    "            if scores[each_class] > max_score or classified == \"\":\n",
    "                max_score = scores[each_class]\n",
    "                classified = each_class\n",
    "\n",
    "        return str(classified)\n",
    "\n",
    "    def featurize(self, data):\n",
    "        \"\"\"\n",
    "        we use this format to make implementation of this class more straightforward and to be\n",
    "        consistent with what you see in nltk\n",
    "        Parameters:\n",
    "          data - str like \"I loved the hotel\"\n",
    "        Return: a list of tuples linking features to values\n",
    "        for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "        \"\"\"\n",
    "        num_positive = 0\n",
    "        num_negative = 0\n",
    "        num_profanity = 0\n",
    "        \n",
    "        num_embedding_pos = 0\n",
    "        num_embedding_neg = 0\n",
    "\n",
    "        words = data\n",
    "\n",
    "        # Number of positive lexicon\n",
    "        for word in words:\n",
    "            if word in self.positive_words:\n",
    "                num_positive += 1\n",
    "            if word in self.negative_words:\n",
    "                num_negative += 1\n",
    "            if word in self.profanity:\n",
    "                num_profanity += num_profanity\n",
    "                \n",
    "            most_similar = self.word_embeddings.most_similar(positive=[word])\n",
    "            \n",
    "            for w in most_similar:\n",
    "                if w[0] in self.positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in self.negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "                \n",
    "                if w[0] in self.profanity:\n",
    "                    num_profanity += 1\n",
    "\n",
    "        difference = num_positive - num_negative\n",
    "        embed_difference = num_embedding_pos - num_embedding_neg\n",
    "        \n",
    "        return [\n",
    "            # (\"num_positive\", num_positive),\n",
    "            # ('num_negative', num_negative),\n",
    "            ('difference', difference),\n",
    "            ('profanity', num_profanity),\n",
    "            ('difference embeddings', embed_difference),\n",
    "            ('bias', 1)\n",
    "        ]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Logistic Regression - Stopwords Removed - 5 features\"\n",
    "\n",
    "    def describe_experiments(self):\n",
    "        s = \"\"\"\n",
    "                My improved Training model is a logistic regression model that learns using stochastic gradient descent.\n",
    "                I experimented with 5 different features, and also normalized the training and testing data by removing \n",
    "                stop words.\n",
    "                \n",
    "                I initially started with Positive lexicons, negative lexicons, difference of the two, whether an \n",
    "                exclamation mark is in it, and whether it has the words \"another, typical\" in them. \n",
    "                \n",
    "                Sometimes, because of the randomized weights and training data, I will achieve a lower than desirable \n",
    "                value for our scores. This is because of the low number of iterations that I have, but this can be \n",
    "                solved by increasing the number of iterations that we do.\n",
    "                \n",
    "                # Experiment 1: Only count Positive and negative lexicons as features\n",
    "                Precision: 0.7313\n",
    "                Recall: 0.467\n",
    "                F1: 0.570\n",
    "                \n",
    "                # Experiment 2: Only count difference as feature\n",
    "                Precision: 0.554\n",
    "                Recall: 0.933\n",
    "                F1: 0.695\n",
    "                \n",
    "                # Experiment 3: Only count difference, exclamation, typical/difference as feature\n",
    "                Precision: 0.551\n",
    "                Recall: 0.933\n",
    "                F1: 0.693\n",
    "                \n",
    "                # Experiment 4: Leave out typical/another\n",
    "                Precision: 0.563\n",
    "                Recall: 0.857\n",
    "                F1: 0.679\n",
    "                \n",
    "                # Experiment 5: Leave out exclamation\n",
    "                Precision: 0.560\n",
    "                Recall: 0.848\n",
    "                F1: 0.674\n",
    "                \n",
    "                # Experiment 6: Leave out difference\n",
    "                Precision: 0.703\n",
    "                Recall: 0.610\n",
    "                F1: 0.653\n",
    "                \n",
    "                # Experiment 7: Include stop words\n",
    "                Precision: 0.551\n",
    "                Recall: 0.933\n",
    "                F1: 0.693\n",
    "                \n",
    "            We are running the experiments on the training data without shuffling so that it will be the same. We \n",
    "            conclude that only counting the number of positive and negative lexicon is not as accurate as counting the \n",
    "            difference. We are also getting low precision but high recall, meaning that we are getting a lot of false\n",
    "            positives. \n",
    "            \"\"\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ca632bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n",
      "100 checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Getting the word embeddings\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pre_split = pd.read_csv('tokenized_df.csv')\n",
    "\n",
    "tokenized_train, tokenized_test = train_test_split(pre_split[:50000], test_size=0.2, random_state=44)\n",
    "\n",
    "positive_words = read_text(\"positive-words.txt\")\n",
    "negative_words = read_text(\"negative-words.txt\")\n",
    "profanity = list(pd.read_csv('profanity_en.csv')['text'])\n",
    "\n",
    "word_embeddings = None\n",
    "\n",
    "\n",
    "\n",
    "def featurize(data):\n",
    "    \"\"\"\n",
    "    we use this format to make implementation of this class more straightforward and to be\n",
    "    consistent with what you see in nltk\n",
    "    Parameters:\n",
    "      data - str like \"I loved the hotel\"\n",
    "    Return: a list of tuples linking features to values\n",
    "    for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "    \"\"\"\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    num_profanity = 0\n",
    "\n",
    "    num_embedding_pos = 0\n",
    "    num_embedding_neg = 0\n",
    "\n",
    "    words = data\n",
    "\n",
    "    # Number of positive lexicon\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            num_positive += 1\n",
    "        if word in negative_words:\n",
    "            num_negative += 1\n",
    "        if word in profanity:\n",
    "            num_profanity += num_profanity\n",
    "        \n",
    "        try:\n",
    "            most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "            for w in most_similar:\n",
    "                if w[0] in positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "\n",
    "                if w[0] in profanity:\n",
    "                    num_profanity += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    difference = num_positive - num_negative\n",
    "    embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "    return [difference, num_profanity, embed_difference]\n",
    "\n",
    "fixed_train = []\n",
    "fixed_test = []\n",
    "\n",
    "fixed_train_label = []\n",
    "fixed_test_label = []\n",
    "\n",
    "word_start = '<s>'\n",
    "word_end = '</s>'\n",
    "\n",
    "token_list_train = list(tokenized_train['tokens'])\n",
    "token_list_train_label = list(tokenized_train['label'])\n",
    "\n",
    "token_list_test = list(tokenized_test['tokens'])\n",
    "token_list_test_label = list(tokenized_test['label'])\n",
    "\n",
    "for index in range(len(token_list_train)):\n",
    "    fixed_sentence = [word_start]\n",
    "    sentence = token_list_train[index]\n",
    "    \n",
    "    for s in sentence[1: -1].split(' '):\n",
    "        fixed_sentence.append(s[1: -2])\n",
    "    \n",
    "    fixed_sentence.append(word_end)\n",
    "    fixed_train.append(fixed_sentence)\n",
    "\n",
    "for index in range(len(token_list_test)):\n",
    "    fixed_sentence = [word_start]\n",
    "    sentence = token_list_test[index]\n",
    "    \n",
    "    for s in sentence[1: -1].split(' '):\n",
    "        fixed_sentence.append(s[1: -2])\n",
    "    \n",
    "    fixed_sentence.append(word_end)\n",
    "    fixed_test.append(fixed_sentence)\n",
    "\n",
    "if os.path.isfile('word2vec.txt'):\n",
    "    word_embeddings = keyedvectors.KeyedVectors.load_word2vec_format('word2vec.txt', binary=False)\n",
    "else:\n",
    "    word_embeddings = Word2Vec(fixed_train, sg=1, window=5, vector_size=200, min_count=1)\n",
    "    word_embeddings.wv.save_word2vec_format('word2vec.txt', binary=False)\n",
    "    \n",
    "# Create List of features [[features...], ...]\n",
    "# labels = [y1, y2, ...]\n",
    "\n",
    "train_features = []\n",
    "\n",
    "count = 0\n",
    "for i in fixed_train:\n",
    "    train_features.append(featurize(i))\n",
    "    \n",
    "    if count % 100 == 0:\n",
    "        print('100 checkpoint')\n",
    "    count += 1\n",
    "    \n",
    "test_features = []\n",
    "\n",
    "for i in fixed_test:\n",
    "    test_features.append(featurize(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3bdb1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 -1  0 ...  1 -1  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5419"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "# with open('test_features.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features, f)\n",
    "#     f.close()\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train_features, token_list_train_label)\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "print(predictions)\n",
    "\n",
    "model.score(test_features, token_list_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa5fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
