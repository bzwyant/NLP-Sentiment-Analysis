{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 14:20:24.778106: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 14:20:25.090762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-14 14:20:25.090777: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-14 14:20:26.015547: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-14 14:20:26.015717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-14 14:20:26.015726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "  1    15830\n",
      " 0    13142\n",
      "-1     8277\n",
      "Name: category, dtype: int64\n",
      "df2\n",
      " neutral     1430\n",
      "positive    1103\n",
      "negative    1001\n",
      "Name: sentiment, dtype: int64\n",
      "df3\n",
      " neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: sentiment, dtype: int64\n",
      "df4\n",
      "  1.0    72250\n",
      " 0.0    55213\n",
      "-1.0    35510\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read in the data\n",
    "df1 = pd.read_csv('data/Reddit_Data.csv',encoding_errors='ignore')\n",
    "df2 = pd.read_csv('data/test.csv',encoding_errors='ignore')\n",
    "df3 = pd.read_csv('data/train.csv',encoding_errors='ignore')\n",
    "df4 = pd.read_csv('data/Twitter_Data.csv',encoding_errors='ignore')\n",
    "# df5 = pd.read_csv('data/train.tsv',encoding_errors='ignore', delimiter='\\t')\n",
    "# df6 = pd.read_csv('data/training.1600000.processed.noemoticon.csv',encoding_errors='ignore')\n",
    "\n",
    "# print out value counts for sentiment or category column of each dataframe\n",
    "print(\"df1\\n\",df1[\"category\"].value_counts())\n",
    "print(\"df2\\n\",df2[\"sentiment\"].value_counts())\n",
    "print(\"df3\\n\",df3[\"sentiment\"].value_counts())\n",
    "print(\"df4\\n\",df4[\"category\"].value_counts())\n",
    "# print(\"df4\\n\",df5[\"Sentiment\"].value_counts())\n",
    "# print(\"df6\\n\",df6[\"polarity of tweet\"].value_counts())\n",
    "\n",
    "\n",
    "# df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_std (37149, 2)\n",
      "df2_std (3534, 2)\n",
      "df3_std (27480, 2)\n",
      "df4_std (162969, 2)\n",
      "combined_df (231132, 2)\n"
     ]
    }
   ],
   "source": [
    "df1_std = df1.copy()\n",
    "df2_std = df2.copy()\n",
    "df3_std = df3.copy()\n",
    "df4_std = df4.copy()\n",
    "# df5_std = df5.copy()\n",
    "# df6_std = df6.copy()\n",
    "\n",
    "df1_std.columns = [\"text\",\"label\"]\n",
    "df1_std.dropna(axis='index',inplace=True)\n",
    "\n",
    "df2_std = df2_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df2_std.dropna(axis='index',inplace=True)\n",
    "df2_std.columns = [\"text\",\"label\"]\n",
    "df2_std[\"label\"] = df2_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "\n",
    "df3_std = df3_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df3_std.dropna(axis='index',inplace=True)\n",
    "df3_std.columns = [\"text\",\"label\"]\n",
    "df3_std[\"label\"] = df3_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "df3_std\n",
    "\n",
    "df4_std = df4_std.loc[:,[\"clean_text\",\"category\"]]\n",
    "df4_std.dropna(axis='index',inplace=True)\n",
    "df4_std.columns = [\"text\",\"label\"]\n",
    "df4_std[\"label\"] = df4_std[\"label\"].map({1.0:'1',0.0:'0',-1.0:'-1'})\n",
    "df4_std\n",
    "\n",
    "# df5_std = df5_std.loc[:,[\"text\",\"Sentiment\"]]\n",
    "# df5_std.columns = [\"text\",\"label\"]\n",
    "# df5_std[\"label\"] = df5_std[\"label\"].map({\"positive\":1,\"neutral\":0,\"negative\":-1})\n",
    "# df5_std\n",
    "\n",
    "# df6_std = df6_std.loc[:,[\"text of the tweet\",\"polarity of tweet\"]]\n",
    "# df6_std.dropna(axis='index',inplace=True)\n",
    "# df6_std.columns = [\"text\",\"label\"]\n",
    "# df6_std[\"label\"] = df6_std[\"label\"].map({4:'1',1:'-1'})\n",
    "# df6_std\n",
    "\n",
    "# combine df1_std and df2_std dataframes\n",
    "combined_df = pd.concat([df1_std,df2_std,df3_std,df4_std],axis=0)\n",
    "combined_df\n",
    "\n",
    "# print out the shapes of all dataframes\n",
    "print(\"df1_std\",df1_std.shape)\n",
    "print(\"df2_std\",df2_std.shape)\n",
    "print(\"df3_std\",df3_std.shape)\n",
    "print(\"df4_std\",df4_std.shape)\n",
    "# print(\"df5_std\",df5_std.shape)\n",
    "print(\"combined_df\",combined_df.shape)\n",
    "\n",
    "combined_df\n",
    "combined_df.text = combined_df.text.str.lower()\n",
    "\n",
    "combined_df.to_csv('data/combined_df.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def replace_unks(tokens, counter: Counter, replaceToken='UNK', threshold=1):\n",
    "    \"\"\"\n",
    "    will replace all words with frequency <= 1,\n",
    "    including those which haven't been seen yet (i.e. if this word is not in `wordfreq`)\n",
    "    \"\"\"\n",
    "    return [w if counter[w] > threshold else replaceToken for w in tokens]\n",
    "\n",
    "\n",
    "data = combined_df.text.apply(nltk.word_tokenize)\n",
    "\n",
    "word_counter = Counter([token for line in data for token in line])\n",
    "print(len(word_counter))\n",
    "\n",
    "tokens_with_unks=data.apply(replace_unks, args=(word_counter,'UNK'))\n",
    "\n",
    "tokenized_df = combined_df.copy()\n",
    "tokenized_df.columns = [\"tokens\",\"label\"]\n",
    "tokenized_df[\"tokens\"] = tokens_with_unks\n",
    "\n",
    "\n",
    "tokenized_df.to_csv('tokenized_df.csv',index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in (25,50,100,200):\n",
    "\n",
    "    # Train the Word2Vec model from Gensim. \n",
    "    sg = 1\n",
    "    window = 5\n",
    "    vector_size = size\n",
    "    min_count = 1\n",
    "\n",
    "    embeddings = Word2Vec(sentences=tokens_with_unks, vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=multiprocessing.cpu_count())\n",
    "    # save file in txt format, then load later if you wish.\n",
    "    embeddings.wv.save_word2vec_format(f'embeddings_unks_{size}.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fd6a938f2e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import keyedvectors,KeyedVectors\n",
    "# from gensim.test.utils import datapath\n",
    "# from gensim.models.keyedvectors.Word2VecKeyedVectors import load_word2vec_format \n",
    "embedding_25 = KeyedVectors.load_word2vec_format('embeddings_unks_25.txt', binary=False)\n",
    "embedding_25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
