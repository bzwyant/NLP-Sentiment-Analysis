{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "  1    15830\n",
      " 0    13142\n",
      "-1     8277\n",
      "Name: category, dtype: int64\n",
      "df2\n",
      " neutral     1430\n",
      "positive    1103\n",
      "negative    1001\n",
      "Name: sentiment, dtype: int64\n",
      "df3\n",
      " neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: sentiment, dtype: int64\n",
      "df4\n",
      "  1.0    72250\n",
      " 0.0    55213\n",
      "-1.0    35510\n",
      "Name: category, dtype: int64\n",
      "df6\n",
      " 0    799996\n",
      "4    248576\n",
      "Name: polarity of tweet, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweet</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048567</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186342</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Madelinedugganx</td>\n",
       "      <td>My GrandMa is making Dinenr with my Mum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048568</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186409</td>\n",
       "      <td>Fri May 29 07:33:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OffRoad_Dude</td>\n",
       "      <td>Mid-morning snack time... A bowl of cheese noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048569</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186429</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Falchion</td>\n",
       "      <td>@ShaDeLa same here  say it like from the Termi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186445</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jonasobsessedx</td>\n",
       "      <td>@DestinyHope92 im great thaanks  wbuu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186607</td>\n",
       "      <td>Fri May 29 07:33:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sugababez</td>\n",
       "      <td>cant wait til her date this weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048572 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
       "0                        0       1467810672  Mon Apr 06 22:19:49 PDT 2009   \n",
       "1                        0       1467810917  Mon Apr 06 22:19:53 PDT 2009   \n",
       "2                        0       1467811184  Mon Apr 06 22:19:57 PDT 2009   \n",
       "3                        0       1467811193  Mon Apr 06 22:19:57 PDT 2009   \n",
       "4                        0       1467811372  Mon Apr 06 22:20:00 PDT 2009   \n",
       "...                    ...              ...                           ...   \n",
       "1048567                  4       1960186342  Fri May 29 07:33:44 PDT 2009   \n",
       "1048568                  4       1960186409  Fri May 29 07:33:43 PDT 2009   \n",
       "1048569                  4       1960186429  Fri May 29 07:33:44 PDT 2009   \n",
       "1048570                  4       1960186445  Fri May 29 07:33:44 PDT 2009   \n",
       "1048571                  4       1960186607  Fri May 29 07:33:45 PDT 2009   \n",
       "\n",
       "            query             user  \\\n",
       "0        NO_QUERY    scotthamilton   \n",
       "1        NO_QUERY         mattycus   \n",
       "2        NO_QUERY          ElleCTF   \n",
       "3        NO_QUERY           Karoli   \n",
       "4        NO_QUERY         joy_wolf   \n",
       "...           ...              ...   \n",
       "1048567  NO_QUERY  Madelinedugganx   \n",
       "1048568  NO_QUERY     OffRoad_Dude   \n",
       "1048569  NO_QUERY         Falchion   \n",
       "1048570  NO_QUERY   jonasobsessedx   \n",
       "1048571  NO_QUERY        sugababez   \n",
       "\n",
       "                                         text of the tweet  \n",
       "0        is upset that he can't update his Facebook by ...  \n",
       "1        @Kenichan I dived many times for the ball. Man...  \n",
       "2          my whole body feels itchy and like its on fire   \n",
       "3        @nationwideclass no, it's not behaving at all....  \n",
       "4                            @Kwesidei not the whole crew   \n",
       "...                                                    ...  \n",
       "1048567           My GrandMa is making Dinenr with my Mum   \n",
       "1048568  Mid-morning snack time... A bowl of cheese noo...  \n",
       "1048569  @ShaDeLa same here  say it like from the Termi...  \n",
       "1048570             @DestinyHope92 im great thaanks  wbuu?  \n",
       "1048571               cant wait til her date this weekend   \n",
       "\n",
       "[1048572 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read in the data\n",
    "df1 = pd.read_csv('data/Reddit_Data.csv',encoding_errors='ignore')\n",
    "df2 = pd.read_csv('data/test.csv',encoding_errors='ignore')\n",
    "df3 = pd.read_csv('data/train.csv',encoding_errors='ignore')\n",
    "df4 = pd.read_csv('data/Twitter_Data.csv',encoding_errors='ignore')\n",
    "# df5 = pd.read_csv('data/train.tsv',encoding_errors='ignore', delimiter='\\t')\n",
    "# df6 = pd.read_csv('data/training.1600000.processed.noemoticon.csv',encoding_errors='ignore')\n",
    "\n",
    "# print out value counts for sentiment or category column of each dataframe\n",
    "print(\"df1\\n\",df1[\"category\"].value_counts())\n",
    "print(\"df2\\n\",df2[\"sentiment\"].value_counts())\n",
    "print(\"df3\\n\",df3[\"sentiment\"].value_counts())\n",
    "print(\"df4\\n\",df4[\"category\"].value_counts())\n",
    "# print(\"df4\\n\",df5[\"Sentiment\"].value_counts())\n",
    "# print(\"df6\\n\",df6[\"polarity of tweet\"].value_counts())\n",
    "\n",
    "\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_std (37149, 2)\n",
      "df2_std (3534, 2)\n",
      "df3_std (27480, 2)\n",
      "df4_std (162969, 2)\n",
      "combined_df (231132, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>231132 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text label\n",
       "0        family mormon have never tried explain them t...     1\n",
       "1       buddhism has very much lot compatible with chr...     1\n",
       "2       seriously don say thing first all they won get...    -1\n",
       "3       what you have learned yours and only yours wha...     0\n",
       "4       for your own benefit you may want read living ...     1\n",
       "...                                                   ...   ...\n",
       "162975  why these 456 crores paid neerav modi not reco...    -1\n",
       "162976  dear rss terrorist payal gawar what about modi...    -1\n",
       "162977  did you cover her interaction forum where she ...     0\n",
       "162978  there big project came into india modi dream p...     0\n",
       "162979  have you ever listen about like gurukul where ...     1\n",
       "\n",
       "[231132 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_std = df1.copy()\n",
    "df2_std = df2.copy()\n",
    "df3_std = df3.copy()\n",
    "df4_std = df4.copy()\n",
    "# df5_std = df5.copy()\n",
    "df6_std = df6.copy()\n",
    "\n",
    "df1_std.columns = [\"text\",\"label\"]\n",
    "df1_std.dropna(axis='index',inplace=True)\n",
    "\n",
    "df2_std = df2_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df2_std.dropna(axis='index',inplace=True)\n",
    "df2_std.columns = [\"text\",\"label\"]\n",
    "df2_std[\"label\"] = df2_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "\n",
    "df3_std = df3_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df3_std.dropna(axis='index',inplace=True)\n",
    "df3_std.columns = [\"text\",\"label\"]\n",
    "df3_std[\"label\"] = df3_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "df3_std\n",
    "\n",
    "df4_std = df4_std.loc[:,[\"clean_text\",\"category\"]]\n",
    "df4_std.dropna(axis='index',inplace=True)\n",
    "df4_std.columns = [\"text\",\"label\"]\n",
    "df4_std[\"label\"] = df4_std[\"label\"].map({1.0:'1',0.0:'0',-1.0:'-1'})\n",
    "df4_std\n",
    "\n",
    "# df5_std = df5_std.loc[:,[\"text\",\"Sentiment\"]]\n",
    "# df5_std.columns = [\"text\",\"label\"]\n",
    "# df5_std[\"label\"] = df5_std[\"label\"].map({\"positive\":1,\"neutral\":0,\"negative\":-1})\n",
    "# df5_std\n",
    "\n",
    "# df6_std = df6_std.loc[:,[\"text of the tweet\",\"polarity of tweet\"]]\n",
    "# df6_std.dropna(axis='index',inplace=True)\n",
    "# df6_std.columns = [\"text\",\"label\"]\n",
    "# df6_std[\"label\"] = df6_std[\"label\"].map({4:'1',1:'-1'})\n",
    "# df6_std\n",
    "\n",
    "# combine df1_std and df2_std dataframes\n",
    "combined_df = pd.concat([df1_std,df2_std,df3_std,df4_std],axis=0)\n",
    "combined_df\n",
    "\n",
    "# print out the shapes of all dataframes\n",
    "print(\"df1_std\",df1_std.shape)\n",
    "print(\"df2_std\",df2_std.shape)\n",
    "print(\"df3_std\",df3_std.shape)\n",
    "print(\"df4_std\",df4_std.shape)\n",
    "# print(\"df5_std\",df5_std.shape)\n",
    "print(\"combined_df\",combined_df.shape)\n",
    "\n",
    "combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "# make all the text lowercase\n",
    "combined_df.text = combined_df.text.str.lower()\n",
    "\n",
    "# save the data as a list of tokenized sentences\n",
    "data = combined_df.text.apply(nltk.word_tokenize)\n",
    "data_lines = data.tolist()\n",
    "\n",
    "## stuff if we wanna do NGRAM related stuff\n",
    "# NGRAM = 3\n",
    "# spooky_lines = data.apply(lambda x: [\"<s>\"]*(NGRAM-1) + x + [\"</s>\"]*(NGRAM-1)).tolist()\n",
    "\n",
    "# with open(f\"spooky_lines_{NGRAM}.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "#     for line in combined_df[\"text\"]:\n",
    "#         f.write(\"<line> \"*(NGRAM-1) + line + \" </line>\"*(NGRAM-1)+\"\\n\")\n",
    "\n",
    "# The dimension of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# you may vary this as you desire\n",
    "# EMBEDDINGS_SIZE = 100\n",
    "for EMBEDDINGS_SIZE in (25,50,100,200):\n",
    "\n",
    "    # Train the Word2Vec model from Gensim. \n",
    "    sg = 1\n",
    "    window = 5\n",
    "    vector_size = EMBEDDINGS_SIZE\n",
    "    min_count = 1\n",
    "\n",
    "    embeddings = Word2Vec(sentences=data_lines, vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=multiprocessing.cpu_count())\n",
    "    # save file in txt format, then load later if you wish.\n",
    "    embeddings.wv.save_word2vec_format(f'embeddings_{EMBEDDINGS_SIZE}.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 151279\n"
     ]
    }
   ],
   "source": [
    "# if you save your Word2Vec as the variable model, this will \n",
    "# print out the vocabulary size\n",
    "print('Vocab size: {}'.format(len(embeddings.wv)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
