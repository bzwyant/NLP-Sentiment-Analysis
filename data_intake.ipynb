{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1\n",
      "  1    15830\n",
      " 0    13142\n",
      "-1     8277\n",
      "Name: category, dtype: int64\n",
      "df2\n",
      " neutral     1430\n",
      "positive    1103\n",
      "negative    1001\n",
      "Name: sentiment, dtype: int64\n",
      "df3\n",
      " neutral     11118\n",
      "positive     8582\n",
      "negative     7781\n",
      "Name: sentiment, dtype: int64\n",
      "df4\n",
      "  1.0    72250\n",
      " 0.0    55213\n",
      "-1.0    35510\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read in the data\n",
    "df1 = pd.read_csv('data/Reddit_Data.csv',encoding_errors='ignore')\n",
    "df2 = pd.read_csv('data/test.csv',encoding_errors='ignore')\n",
    "df3 = pd.read_csv('data/train.csv',encoding_errors='ignore')\n",
    "df4 = pd.read_csv('data/Twitter_Data.csv',encoding_errors='ignore')\n",
    "# df5 = pd.read_csv('data/train.tsv',encoding_errors='ignore', delimiter='\\t')\n",
    "# df6 = pd.read_csv('data/training.1600000.processed.noemoticon.csv',encoding_errors='ignore')\n",
    "\n",
    "# print out value counts for sentiment or category column of each dataframe\n",
    "print(\"df1\\n\",df1[\"category\"].value_counts())\n",
    "print(\"df2\\n\",df2[\"sentiment\"].value_counts())\n",
    "print(\"df3\\n\",df3[\"sentiment\"].value_counts())\n",
    "print(\"df4\\n\",df4[\"category\"].value_counts())\n",
    "# print(\"df4\\n\",df5[\"Sentiment\"].value_counts())\n",
    "# print(\"df6\\n\",df6[\"polarity of tweet\"].value_counts())\n",
    "\n",
    "\n",
    "# df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_std (37149, 2)\n",
      "df2_std (3534, 2)\n",
      "df3_std (27480, 2)\n",
      "df4_std (162969, 2)\n",
      "combined_df (231132, 2)\n"
     ]
    }
   ],
   "source": [
    "df1_std = df1.copy()\n",
    "df2_std = df2.copy()\n",
    "df3_std = df3.copy()\n",
    "df4_std = df4.copy()\n",
    "# df5_std = df5.copy()\n",
    "# df6_std = df6.copy()\n",
    "\n",
    "df1_std.columns = [\"text\",\"label\"]\n",
    "df1_std.dropna(axis='index',inplace=True)\n",
    "\n",
    "df2_std = df2_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df2_std.dropna(axis='index',inplace=True)\n",
    "df2_std.columns = [\"text\",\"label\"]\n",
    "df2_std[\"label\"] = df2_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "\n",
    "df3_std = df3_std.loc[:,[\"text\",\"sentiment\"]]\n",
    "df3_std.dropna(axis='index',inplace=True)\n",
    "df3_std.columns = [\"text\",\"label\"]\n",
    "df3_std[\"label\"] = df3_std[\"label\"].map({\"positive\":'1',\"neutral\":'0',\"negative\":'-1'})\n",
    "df3_std\n",
    "\n",
    "df4_std = df4_std.loc[:,[\"clean_text\",\"category\"]]\n",
    "df4_std.dropna(axis='index',inplace=True)\n",
    "df4_std.columns = [\"text\",\"label\"]\n",
    "df4_std[\"label\"] = df4_std[\"label\"].map({1.0:'1',0.0:'0',-1.0:'-1'})\n",
    "df4_std\n",
    "\n",
    "# df5_std = df5_std.loc[:,[\"text\",\"Sentiment\"]]\n",
    "# df5_std.columns = [\"text\",\"label\"]\n",
    "# df5_std[\"label\"] = df5_std[\"label\"].map({\"positive\":1,\"neutral\":0,\"negative\":-1})\n",
    "# df5_std\n",
    "\n",
    "# df6_std = df6_std.loc[:,[\"text of the tweet\",\"polarity of tweet\"]]\n",
    "# df6_std.dropna(axis='index',inplace=True)\n",
    "# df6_std.columns = [\"text\",\"label\"]\n",
    "# df6_std[\"label\"] = df6_std[\"label\"].map({4:'1',1:'-1'})\n",
    "# df6_std\n",
    "\n",
    "# combine df1_std and df2_std dataframes\n",
    "combined_df = pd.concat([df1_std,df2_std,df3_std,df4_std],axis=0)\n",
    "combined_df\n",
    "\n",
    "# print out the shapes of all dataframes\n",
    "print(\"df1_std\",df1_std.shape)\n",
    "print(\"df2_std\",df2_std.shape)\n",
    "print(\"df3_std\",df3_std.shape)\n",
    "print(\"df4_std\",df4_std.shape)\n",
    "# print(\"df5_std\",df5_std.shape)\n",
    "print(\"combined_df\",combined_df.shape)\n",
    "\n",
    "combined_df\n",
    "combined_df.text = combined_df.text.str.lower()\n",
    "\n",
    "combined_df.to_csv('data/combined_df.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151279\n",
      "56994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import defaultdict, Counter\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "def replace_unks(tokens, counter: Counter, replaceToken='UNK', threshold=1):\n",
    "    \"\"\"\n",
    "    will replace all words with frequency <= 1,\n",
    "    including those which haven't been seen yet (i.e. if this word is not in `wordfreq`)\n",
    "    \"\"\"\n",
    "    return [w if counter[w] > threshold else replaceToken for w in tokens]\n",
    "\n",
    "\n",
    "data = combined_df.text.apply(nltk.word_tokenize)\n",
    "\n",
    "word_counter = Counter([token for line in data for token in line])\n",
    "print(len(word_counter))\n",
    "\n",
    "tokens_with_unks=data.apply(replace_unks, args=(word_counter,'UNK'))\n",
    "\n",
    "tokenized_df = combined_df.copy()\n",
    "tokenized_df.columns = [\"tokens\",\"label\"]\n",
    "tokenized_df[\"tokens\"] = tokens_with_unks\n",
    "\n",
    "\n",
    "tokenized_df.to_csv('tokenized_df.csv',index=False)\n",
    "word_counter2 = Counter([token for line in tokenized_df[\"tokens\"] for token in line])\n",
    "print(len(word_counter2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save tokenized_df to pickle file\n",
    "with open('tokenized_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenized_df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56994\n"
     ]
    }
   ],
   "source": [
    "## COPY THIS TO LOAD tokenized_df.csv back into notebook as dataframe #\n",
    "# load tokenized_df from pickle file\n",
    "with open('tokenized_df.pickle', 'rb') as handle:\n",
    "    loaded_tokenized_df = pickle.load(handle)\n",
    "\n",
    "# make sure word count is the same as before saving to file\n",
    "word_counter3 = Counter([token for line in loaded_tokenized_df[\"tokens\"] for token in line])\n",
    "print(len(word_counter3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for size in tqdm([25,50,100,200]):\n",
    "\n",
    "    # Train the Word2Vec model from Gensim. \n",
    "    sg = 1\n",
    "    window = 5\n",
    "    vector_size = size\n",
    "    min_count = 1\n",
    "    if os.path.exists(f'embeddings_unks_{size}.txt'):\n",
    "        embeddings = KeyedVectors.load_word2vec_format(f'embeddings_unks_{size}.txt', binary=False)\n",
    "        continue \n",
    "    embeddings = Word2Vec(sentences=tokens_with_unks, vector_size=vector_size, window=window, min_count=min_count, sg=sg, workers=multiprocessing.cpu_count())\n",
    "    # save file in txt format, then load later if you wish.\n",
    "    embeddings.wv.save_word2vec_format(f'embeddings_unks_{size}.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7ff90df622c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import keyedvectors,KeyedVectors\n",
    "# from gensim.test.utils import datapath\n",
    "# from gensim.models.keyedvectors.Word2VecKeyedVectors import load_word2vec_format \n",
    "embedding_25 = KeyedVectors.load_word2vec_format('embeddings_unks_25.txt', binary=False)\n",
    "embedding_25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
