{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- https://towardsdatascience.com/multiclass-text-classification-using-keras-to-predict-emotions-comparison-with-and-without-word-5ef0a5eaa1a0\n",
    "\n",
    "- https://medium.com/analytics-vidhya/train-keras-model-with-large-dataset-batch-training-6b3099fdf366"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 15:18:44.414070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras_rnn import SentimentLSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# visualizations\n",
    "import plotly.express as px\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenized_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized_df.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m embedding_25 \u001B[38;5;241m=\u001B[39m \u001B[43mKeyedVectors\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43membeddings_unks_25.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m embedding_25\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/NN-Language-Model/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1719\u001B[0m, in \u001B[0;36mKeyedVectors.load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001B[0m\n\u001B[1;32m   1672\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m   1673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_word2vec_format\u001B[39m(\n\u001B[1;32m   1674\u001B[0m         \u001B[38;5;28mcls\u001B[39m, fname, fvocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m'\u001B[39m, unicode_errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1675\u001B[0m         limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, datatype\u001B[38;5;241m=\u001B[39mREAL, no_header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1676\u001B[0m     ):\n\u001B[1;32m   1677\u001B[0m     \u001B[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001B[39;00m\n\u001B[1;32m   1678\u001B[0m \n\u001B[1;32m   1679\u001B[0m \u001B[38;5;124;03m    Warnings\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1717\u001B[0m \n\u001B[1;32m   1718\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1719\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load_word2vec_format\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1720\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbinary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1721\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlimit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mno_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1722\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/NN-Language-Model/lib/python3.10/site-packages/gensim/models/keyedvectors.py:2069\u001B[0m, in \u001B[0;36m_load_word2vec_format\u001B[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001B[0m\n\u001B[1;32m   2065\u001B[0m         _word2vec_read_binary(\n\u001B[1;32m   2066\u001B[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001B[1;32m   2067\u001B[0m         )\n\u001B[1;32m   2068\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2069\u001B[0m         \u001B[43m_word2vec_read_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcounts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvector_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2070\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(kv):\n\u001B[1;32m   2071\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[1;32m   2072\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mduplicate words detected, shrinking matrix size from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   2073\u001B[0m         kv\u001B[38;5;241m.\u001B[39mvectors\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlen\u001B[39m(kv),\n\u001B[1;32m   2074\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/NN-Language-Model/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1974\u001B[0m, in \u001B[0;36m_word2vec_read_text\u001B[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[1;32m   1972\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m line \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m   1973\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1974\u001B[0m word, weights \u001B[38;5;241m=\u001B[39m \u001B[43m_word2vec_line_to_vector\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatatype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43municode_errors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1975\u001B[0m _add_word_to_kv(kv, counts, word, weights, vocab_size)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/NN-Language-Model/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1980\u001B[0m, in \u001B[0;36m_word2vec_line_to_vector\u001B[0;34m(line, datatype, unicode_errors, encoding)\u001B[0m\n\u001B[1;32m   1978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_word2vec_line_to_vector\u001B[39m(line, datatype, unicode_errors, encoding):\n\u001B[1;32m   1979\u001B[0m     parts \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mto_unicode(line\u001B[38;5;241m.\u001B[39mrstrip(), encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39municode_errors)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1980\u001B[0m     word, weights \u001B[38;5;241m=\u001B[39m parts[\u001B[38;5;241m0\u001B[39m], [datatype(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m parts[\u001B[38;5;241m1\u001B[39m:]]\n\u001B[1;32m   1981\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m word, weights\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/NN-Language-Model/lib/python3.10/site-packages/gensim/models/keyedvectors.py:1980\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_word2vec_line_to_vector\u001B[39m(line, datatype, unicode_errors, encoding):\n\u001B[1;32m   1979\u001B[0m     parts \u001B[38;5;241m=\u001B[39m utils\u001B[38;5;241m.\u001B[39mto_unicode(line\u001B[38;5;241m.\u001B[39mrstrip(), encoding\u001B[38;5;241m=\u001B[39mencoding, errors\u001B[38;5;241m=\u001B[39municode_errors)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1980\u001B[0m     word, weights \u001B[38;5;241m=\u001B[39m parts[\u001B[38;5;241m0\u001B[39m], [\u001B[43mdatatype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m parts[\u001B[38;5;241m1\u001B[39m:]]\n\u001B[1;32m   1981\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m word, weights\n",
      "\u001B[0;31mValueError\u001B[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "tokenized_df = pd.read_csv('tokenized_df.csv')\n",
    "embedding_25 = KeyedVectors.load_word2vec_format('embeddings_unks_25.txt', binary=False)\n",
    "embedding_25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_list = list(embedding_25.key_to_index.keys())\n",
    "print(len(vocab_list))\n",
    "\n",
    "def remove_non_vocab_words(text: list, vocab):\n",
    "    for i in tqdm(range(len(text))):\n",
    "        text[i] = [word for word in text[i] if word in vocab]\n",
    "    return text\n",
    "\n",
    "POS_LABEL = 0\n",
    "NEUTRAL_LABEL = 1\n",
    "NEG_LABEL = 2\n",
    "\n",
    "train_data = tokenized_df.sample(frac=0.7,random_state=200)\n",
    "train_data['label'] = train_data['label'].map({'1':POS_LABEL,'0':NEUTRAL_LABEL,'-1':NEG_LABEL})\n",
    "train_features, train_labels = train_data.tokens, tf.one_hot(np.asarray(train_data['label']), 3)\n",
    "\n",
    "test_data = tokenized_df.drop(train_data.index)\n",
    "validation_data = test_data.sample(frac=0.5,random_state=200)\n",
    "test_data = test_data.drop(validation_data.index)\n",
    "\n",
    "validation_data['label'] = validation_data['label'].astype('category')\n",
    "validation_data['label_cat'] = validation_data['label'].cat.codes\n",
    "validation_features, validation_labels = validation_data.tokens, tf.one_hot(validation_data['label_cat'], 3)\n",
    "\n",
    "test_data['label'] = test_data['label'].astype('category')\n",
    "test_data['label_cat'] = test_data['label'].cat.codes\n",
    "test_features, test_labels = test_data.tokens, tf.one_hot(test_data['label_cat'], 3)\n",
    "\n",
    "# tokenized_sequences = [remove_non_vocab_words(line, vocab_list) for line in tqdm(data_lines)]\n",
    "print(train_labels[:100])\n",
    "train_data.head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_matrix = embedding_25[embedding_25.key_to_index.keys()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot review length distribution\n",
    "\n",
    "review_lengths = [len(x) for x in tokenized_df['tokens']]\n",
    "length_mean = np.mean(review_lengths)\n",
    "length_std = np.std(review_lengths)\n",
    "# remove outliers whose length is very large\n",
    "review_lengths = [x for x in review_lengths if x < length_mean + 2*length_std]\n",
    "fig = px.histogram(x=review_lengths, labels={'x':'Review Length'}, title=\"Review Length Distribution\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_seq_len = 40\n",
    "\n",
    "def sequences_to_token_indexes(w2v_model, list_features):\n",
    "    indexed_features = []\n",
    "    for sentence in tqdm(list_features):\n",
    "        indexed_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                indexed_sentence.append(w2v_model.key_to_index[word])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        indexed_features.append(indexed_sentence)\n",
    "    return indexed_features\n",
    "\n",
    "indexed_train_features = sequences_to_token_indexes(wv_25, train_features)\n",
    "indexed_validation_features = sequences_to_token_indexes(wv_25, validation_features)\n",
    "indexed_test_features = sequences_to_token_indexes(wv_25, test_features)\n",
    "\n",
    "padded_train = pad_sequences(indexed_train_features, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "padded_validation = pad_sequences(indexed_validation_features, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "padded_test = pad_sequences(indexed_test_features, maxlen=max_seq_len, padding='post', truncating='post')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create batches\n",
    "\n",
    "def batch_generator(features, labels, batch_size):\n",
    "    num_batches = len(features) // batch_size\n",
    "    for batch in range(num_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "# def load_data(features, labels, batch_size):\n",
    "#     dataset = tf.data.Dataset.from_generator(\n",
    "#         lambda: batch_generator(features, labels, batch_size),\n",
    "#         output_types=(tf.int32, tf.int32),\n",
    "#         output_shapes=([None, max_seq_len], [None, 3])\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "batch_size = 64\n",
    "training_batch_generator = batch_generator(padded_train, train_labels, batch_size)\n",
    "validation_batch_generator = batch_generator(padded_validation, validation_labels, batch_size)\n",
    "testing_batch_generator = batch_generator(padded_test, test_labels, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/lstm_with_w2v.hdf5',\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "]\n",
    "\n",
    "print(len(vocab_list))\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "model = SentimentLSTM(vocab_size=len(vocab_list),\n",
    "                      output_dim=25,\n",
    "                      weights=embedding_matrix,\n",
    "                      max_seq_length=max_seq_len)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# storing model training details to analyze later\n",
    "padded_train = np.asarray(padded_train).astype('float32')\n",
    "train_labels = np.asarray(train_labels).astype('float32')\n",
    "\n",
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "print(padded_train[0].shape)\n",
    "print(train_labels[0].shape)\n",
    "\n",
    "# history = model.fit(padded_train,\n",
    "#                     train_labels,\n",
    "#                     validation_split=0.33,\n",
    "#                     callbacks=callbacks,\n",
    "#                     epochs=3)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    training_batch_generator,\n",
    "    steps_per_epoch=len(padded_train) // batch_size,\n",
    "    epochs=3,\n",
    "    validation_data=validation_batch_generator,\n",
    "    validation_steps=len(padded_test) // batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_one_hot = model.predict(padded_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
