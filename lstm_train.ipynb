{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- https://towardsdatascience.com/multiclass-text-classification-using-keras-to-predict-emotions-comparison-with-and-without-word-5ef0a5eaa1a0\n",
    "\n",
    "- https://medium.com/analytics-vidhya/train-keras-model-with-large-dataset-batch-training-6b3099fdf366"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 15:18:44.414070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "\n",
    "# Keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras_rnn import SentimentLSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# visualizations\n",
    "import plotly.express as px\n",
    "\n",
    "# utils\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_list = list(wv_25.key_to_index.keys())\n",
    "print(len(vocab_list))\n",
    "\n",
    "def remove_non_vocab_words(text: list, vocab):\n",
    "    for i in tqdm(range(len(text))):\n",
    "        text[i] = [word for word in text[i] if word in vocab]\n",
    "    return text\n",
    "\n",
    "POS_LABEL = 0\n",
    "NEUTRAL_LABEL = 1\n",
    "NEG_LABEL = 2\n",
    "\n",
    "train_data = combined_df.sample(frac=0.7,random_state=200)\n",
    "train_data['label'] = train_data['label'].map({'1':POS_LABEL,'0':NEUTRAL_LABEL,'-1':NEG_LABEL})\n",
    "train_features, train_labels = train_data.tokens, tf.one_hot(np.asarray(train_data['label']), 3)\n",
    "\n",
    "test_data = combined_df.drop(train_data.index)\n",
    "validation_data = test_data.sample(frac=0.5,random_state=200)\n",
    "test_data = test_data.drop(validation_data.index)\n",
    "\n",
    "validation_data['label'] = validation_data['label'].astype('category')\n",
    "validation_data['label_cat'] = validation_data['label'].cat.codes\n",
    "validation_features, validation_labels = validation_data.tokens, tf.one_hot(validation_data['label_cat'], 3)\n",
    "\n",
    "test_data['label'] = test_data['label'].astype('category')\n",
    "test_data['label_cat'] = test_data['label'].cat.codes\n",
    "test_features, test_labels = test_data.tokens, tf.one_hot(test_data['label_cat'], 3)\n",
    "\n",
    "# tokenized_sequences = [remove_non_vocab_words(line, vocab_list) for line in tqdm(data_lines)]\n",
    "print(train_labels[:100])\n",
    "train_data.head(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_matrix = wv_25[wv_25.key_to_index.keys()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot review length distribution\n",
    "\n",
    "review_lengths = [len(x) for x in combined_df['tokens']]\n",
    "length_mean = np.mean(review_lengths)\n",
    "length_std = np.std(review_lengths)\n",
    "# remove outliers whose length is very large\n",
    "review_lengths = [x for x in review_lengths if x < length_mean + 2*length_std]\n",
    "fig = px.histogram(x=review_lengths, labels={'x':'Review Length'}, title=\"Review Length Distribution\")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_seq_len = 40\n",
    "\n",
    "def sequences_to_token_indexes(w2v_model, list_features):\n",
    "    indexed_features = []\n",
    "    for sentence in tqdm(list_features):\n",
    "        indexed_sentence = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                indexed_sentence.append(w2v_model.key_to_index[word])\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        indexed_features.append(indexed_sentence)\n",
    "    return indexed_features\n",
    "\n",
    "indexed_train_features = sequences_to_token_indexes(wv_25, train_features)\n",
    "indexed_validation_features = sequences_to_token_indexes(wv_25, validation_features)\n",
    "indexed_test_features = sequences_to_token_indexes(wv_25, test_features)\n",
    "\n",
    "padded_train = pad_sequences(indexed_train_features, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "padded_validation = pad_sequences(indexed_validation_features, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "padded_test = pad_sequences(indexed_test_features, maxlen=max_seq_len, padding='post', truncating='post')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create batches\n",
    "\n",
    "def batch_generator(features, labels, batch_size):\n",
    "    num_batches = len(features) // batch_size\n",
    "    for batch in range(num_batches):\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "# def load_data(features, labels, batch_size):\n",
    "#     dataset = tf.data.Dataset.from_generator(\n",
    "#         lambda: batch_generator(features, labels, batch_size),\n",
    "#         output_types=(tf.int32, tf.int32),\n",
    "#         output_shapes=([None, max_seq_len], [None, 3])\n",
    "#     )\n",
    "#     return dataset\n",
    "\n",
    "batch_size = 64\n",
    "training_batch_generator = batch_generator(padded_train, train_labels, batch_size)\n",
    "validation_batch_generator = batch_generator(padded_validation, validation_labels, batch_size)\n",
    "testing_batch_generator = batch_generator(padded_test, test_labels, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the model\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/lstm_with_w2v.hdf5',\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "]\n",
    "\n",
    "print(len(vocab_list))\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "model = SentimentLSTM(vocab_size=len(vocab_list),\n",
    "                      output_dim=25,\n",
    "                      weights=embedding_matrix,\n",
    "                      max_seq_length=max_seq_len)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# storing model training details to analyze later\n",
    "padded_train = np.asarray(padded_train).astype('float32')\n",
    "train_labels = np.asarray(train_labels).astype('float32')\n",
    "\n",
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]\n",
    "print(padded_train[0].shape)\n",
    "print(train_labels[0].shape)\n",
    "\n",
    "# history = model.fit(padded_train,\n",
    "#                     train_labels,\n",
    "#                     validation_split=0.33,\n",
    "#                     callbacks=callbacks,\n",
    "#                     epochs=3)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    training_batch_generator,\n",
    "    steps_per_epoch=len(padded_train) // batch_size,\n",
    "    epochs=3,\n",
    "    validation_data=validation_batch_generator,\n",
    "    validation_steps=len(padded_test) // batch_size,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_one_hot = model.predict(padded_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
