{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a105ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from gensim.models import Word2Vec, keyedvectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# UNZIP THE WORD2VEC200.txt.zip File\n",
    "\n",
    "def read_text(filepath, dictionary=None):\n",
    "    \"\"\"\n",
    "    gets the text and add it to the dictionary\n",
    "    Args:\n",
    "        filepath: the filepath of the text file|\n",
    "        dictionary: the dictionary storage for True to get quick lookup\n",
    "\n",
    "    Returns: the dictionary\n",
    "\n",
    "    \"\"\"\n",
    "    if dictionary is None:\n",
    "        dictionary = {}\n",
    "\n",
    "    f = open(filepath, \"r\", encoding='utf8')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line not in dictionary.keys():\n",
    "            dictionary[line] = True\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d603af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_split = pd.read_csv('tokenized_df.csv').sample(n=50000, random_state=44)\n",
    "\n",
    "tokenized_train, tokenized_test = train_test_split(pre_split, test_size=0.2, random_state=44)\n",
    "\n",
    "positive_words = read_text(\"positive-words.txt\")\n",
    "negative_words = read_text(\"negative-words.txt\")\n",
    "profanity = list(pd.read_csv('profanity_en.csv')['text'])\n",
    "\n",
    "word_embeddings = None\n",
    "\n",
    "def featurize50(data, word_embeddings):\n",
    "    \"\"\"\n",
    "    we use this format to make implementation of this class more straightforward and to be\n",
    "    consistent with what you see in nltk\n",
    "    Parameters:\n",
    "      data - str like \"I loved the hotel\"\n",
    "    Return: a list of tuples linking features to values\n",
    "    for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "    \"\"\"\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    num_profanity = 0\n",
    "\n",
    "    num_embedding_pos = 0\n",
    "    num_embedding_neg = 0\n",
    "\n",
    "    words = data\n",
    "\n",
    "    # Number of positive lexicon\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            num_positive += 1\n",
    "        if word in negative_words:\n",
    "            num_negative += 1\n",
    "        if word in profanity:\n",
    "            num_profanity += num_profanity\n",
    "        \n",
    "        try:\n",
    "            most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "            for w in most_similar:\n",
    "                if w[0] in positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "\n",
    "                if w[0] in profanity:\n",
    "                    num_profanity += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    difference = num_positive - num_negative\n",
    "    embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "    return [difference, num_profanity, embed_difference]\n",
    "\n",
    "def featurize25(data, word_embeddings):\n",
    "    \"\"\"\n",
    "    we use this format to make implementation of this class more straightforward and to be\n",
    "    consistent with what you see in nltk\n",
    "    Parameters:\n",
    "      data - str like \"I loved the hotel\"\n",
    "    Return: a list of tuples linking features to values\n",
    "    for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "    \"\"\"\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    num_profanity = 0\n",
    "\n",
    "    num_embedding_pos = 0\n",
    "    num_embedding_neg = 0\n",
    "\n",
    "    words = data\n",
    "\n",
    "    # Number of positive lexicon\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            num_positive += 1\n",
    "        if word in negative_words:\n",
    "            num_negative += 1\n",
    "        if word in profanity:\n",
    "            num_profanity += num_profanity\n",
    "        \n",
    "        try:\n",
    "            most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "            for w in most_similar:\n",
    "                if w[0] in positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "\n",
    "                if w[0] in profanity:\n",
    "                    num_profanity += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    difference = num_positive - num_negative\n",
    "    embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "    return [difference, num_profanity, embed_difference]\n",
    "\n",
    "\n",
    "def featurize100(data, word_embeddings):\n",
    "    \"\"\"\n",
    "    we use this format to make implementation of this class more straightforward and to be\n",
    "    consistent with what you see in nltk\n",
    "    Parameters:\n",
    "      data - str like \"I loved the hotel\"\n",
    "    Return: a list of tuples linking features to values\n",
    "    for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "    \"\"\"\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    num_profanity = 0\n",
    "\n",
    "    num_embedding_pos = 0\n",
    "    num_embedding_neg = 0\n",
    "\n",
    "    words = data\n",
    "\n",
    "    # Number of positive lexicon\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            num_positive += 1\n",
    "        if word in negative_words:\n",
    "            num_negative += 1\n",
    "        if word in profanity:\n",
    "            num_profanity += num_profanity\n",
    "        \n",
    "        try:\n",
    "            most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "            for w in most_similar:\n",
    "                if w[0] in positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "\n",
    "                if w[0] in profanity:\n",
    "                    num_profanity += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    difference = num_positive - num_negative\n",
    "    embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "    return [difference, num_profanity, embed_difference]\n",
    "\n",
    "\n",
    "def featurize200(data, word_embeddings):\n",
    "    \"\"\"\n",
    "    we use this format to make implementation of this class more straightforward and to be\n",
    "    consistent with what you see in nltk\n",
    "    Parameters:\n",
    "      data - str like \"I loved the hotel\"\n",
    "    Return: a list of tuples linking features to values\n",
    "    for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "    \"\"\"\n",
    "    num_positive = 0\n",
    "    num_negative = 0\n",
    "    num_profanity = 0\n",
    "\n",
    "    num_embedding_pos = 0\n",
    "    num_embedding_neg = 0\n",
    "\n",
    "    words = data\n",
    "\n",
    "    # Number of positive lexicon\n",
    "    for word in words:\n",
    "        if word in positive_words:\n",
    "            num_positive += 1\n",
    "        if word in negative_words:\n",
    "            num_negative += 1\n",
    "        if word in profanity:\n",
    "            num_profanity += num_profanity\n",
    "        \n",
    "        try:\n",
    "            most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "            for w in most_similar:\n",
    "                if w[0] in positive_words:\n",
    "                    num_embedding_pos += 1\n",
    "                elif w[0] in negative_words:\n",
    "                    num_embedding_neg += 1\n",
    "\n",
    "                if w[0] in profanity:\n",
    "                    num_profanity += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    difference = num_positive - num_negative\n",
    "    embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "    return [difference, num_profanity, embed_difference]\n",
    "\n",
    "fixed_train = []\n",
    "fixed_test = []\n",
    "\n",
    "fixed_train_label = []\n",
    "fixed_test_label = []\n",
    "\n",
    "word_start = '<s>'\n",
    "word_end = '</s>'\n",
    "\n",
    "token_list_train = list(tokenized_train['tokens'])\n",
    "token_list_train_label = list(tokenized_train['label'])\n",
    "\n",
    "token_list_test = list(tokenized_test['tokens'])\n",
    "token_list_test_label = list(tokenized_test['label'])\n",
    "\n",
    "for index in range(len(token_list_train)):\n",
    "    fixed_sentence = [word_start]\n",
    "    sentence = token_list_train[index]\n",
    "    \n",
    "    for s in sentence[1: -1].split(' '):\n",
    "        fixed_sentence.append(s[1: -2])\n",
    "    \n",
    "    fixed_sentence.append(word_end)\n",
    "    fixed_train.append(fixed_sentence)\n",
    "\n",
    "for index in range(len(token_list_test)):\n",
    "    fixed_sentence = [word_start]\n",
    "    sentence = token_list_test[index]\n",
    "    \n",
    "    for s in sentence[1: -1].split(' '):\n",
    "        fixed_sentence.append(s[1: -2])\n",
    "    \n",
    "    fixed_sentence.append(word_end)\n",
    "    fixed_test.append(fixed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20698929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding 25\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings25 = None\n",
    "\n",
    "# if os.path.isfile('word2vec25.txt'):\n",
    "#     word_embeddings25 = keyedvectors.KeyedVectors.load_word2vec_format('word2vec25.txt', binary=False)\n",
    "# else:\n",
    "#     word_embeddings25 = Word2Vec(vocabulary, sg=1, window=5, vector_size=25, min_count=1)\n",
    "#     word_embeddings25.wv.save_word2vec_format('word2vec25.txt', binary=False)\n",
    "\n",
    "# train_features25 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features25.append(featurize25(i, word_embeddings25))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features25 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features25.append(featurize25(i, word_embeddings25))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "    \n",
    "# with open('./train_features25.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features25, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('./test_features25.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features25, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f535db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Word Embedding 50\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings50 = None\n",
    "\n",
    "# if os.path.isfile('word2vec50.txt'):\n",
    "#     word_embeddings50 = keyedvectors.KeyedVectors.load_word2vec_format('word2vec50.txt', binary=False)\n",
    "# else:\n",
    "#     word_embeddings50 = Word2Vec(vocabulary, sg=1, window=5, vector_size=50, min_count=1)\n",
    "#     word_embeddings50.wv.save_word2vec_format('word2vec50.txt', binary=False)\n",
    "\n",
    "# train_features50 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features50.append(featurize50(i, word_embeddings50))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features50 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features50.append(featurize50(i, word_embeddings50))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "\n",
    "# with open('./train_features50.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features50, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('./test_features50.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features50, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5611b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Word Embedding 100\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings100 = None\n",
    "\n",
    "# if os.path.isfile('word2vec100.txt'):\n",
    "#     word_embeddings100 = keyedvectors.KeyedVectors.load_word2vec_format('word2vec100.txt', binary=False)\n",
    "# else:\n",
    "#     word_embeddings100 = Word2Vec(vocabulary, sg=1, window=5, vector_size=100, min_count=1)\n",
    "#     word_embeddings100.wv.save_word2vec_format('word2vec100.txt', binary=False)\n",
    "\n",
    "# train_features100 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features100.append(featurize100(i, word_embeddings100))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features100 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features100.append(featurize100(i, word_embeddings100))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# with open('./train_features100.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features100, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('./test_features100.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features100, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff38a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Word Embedding 200\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings200 = None\n",
    "\n",
    "# if os.path.isfile('word2vec200.txt'):\n",
    "#     word_embeddings200 = keyedvectors.KeyedVectors.load_word2vec_format('word2vec200.txt', binary=False)\n",
    "# else:\n",
    "#     word_embeddings200 = Word2Vec(vocabulary, sg=1, window=5, vector_size=200, min_count=1)\n",
    "#     word_embeddings200.wv.save_word2vec_format('word2vec200.txt', binary=False)\n",
    "\n",
    "# train_features200 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features200.append(featurize200(i, word_embeddings200))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features200 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features200.append(featurize200(i, word_embeddings200))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# with open('train_features200.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features200, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('test_features200.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features200, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce0de7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GenSim Twitter \n",
    "\n",
    "# def featurize200_twitter(data, word_embeddings):\n",
    "#     \"\"\"\n",
    "#     we use this format to make implementation of this class more straightforward and to be\n",
    "#     consistent with what you see in nltk\n",
    "#     Parameters:\n",
    "#       data - str like \"I loved the hotel\"\n",
    "#     Return: a list of tuples linking features to values\n",
    "#     for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "#     \"\"\"\n",
    "#     num_positive = 0\n",
    "#     num_negative = 0\n",
    "#     num_profanity = 0\n",
    "\n",
    "#     num_embedding_pos = 0\n",
    "#     num_embedding_neg = 0\n",
    "\n",
    "#     words = data\n",
    "\n",
    "#     # Number of positive lexicon\n",
    "#     for word in words:\n",
    "#         if word in positive_words:\n",
    "#             num_positive += 1\n",
    "            \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "#         if word in negative_words:\n",
    "#             num_negative += 1\n",
    "            \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "#         if word in profanity:\n",
    "#             num_profanity += num_profanity\n",
    "        \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#     difference = num_positive - num_negative\n",
    "#     embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "#     return [difference, num_profanity, embed_difference]\n",
    "\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings_twitter200 = api.load('glove-twitter-200')\n",
    "# train_features_twitter200 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features_twitter200.append(featurize200_twitter(i, word_embeddings_twitter200))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features_twitter200 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features_twitter200.append(featurize200_twitter(i, word_embeddings_twitter200))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# with open('./train_features_twitter200.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features_twitter200, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('./test_features_twitter200.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features_twitter200, f)\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c6f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# def featurize100_twitter(data, word_embeddings):\n",
    "#     \"\"\"\n",
    "#     we use this format to make implementation of this class more straightforward and to be\n",
    "#     consistent with what you see in nltk\n",
    "#     Parameters:\n",
    "#       data - str like \"I loved the hotel\"\n",
    "#     Return: a list of tuples linking features to values\n",
    "#     for BoW, a list of tuples linking every word to True [(\"I\", True), (\"loved\", True), (\"it\", True)]\n",
    "#     \"\"\"\n",
    "#     num_positive = 0\n",
    "#     num_negative = 0\n",
    "#     num_profanity = 0\n",
    "\n",
    "#     num_embedding_pos = 0\n",
    "#     num_embedding_neg = 0\n",
    "\n",
    "#     words = data\n",
    "\n",
    "#     # Number of positive lexicon\n",
    "#     for word in words:\n",
    "#         if word in positive_words:\n",
    "#             num_positive += 1\n",
    "            \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "#         if word in negative_words:\n",
    "#             num_negative += 1\n",
    "            \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "#         if word in profanity:\n",
    "#             num_profanity += num_profanity\n",
    "        \n",
    "#             try:\n",
    "#                 most_similar = word_embeddings.most_similar(positive=[word])[:5]\n",
    "\n",
    "#                 for w in most_similar:\n",
    "#                     if w[0] in positive_words:\n",
    "#                         num_embedding_pos += 1\n",
    "#                     elif w[0] in negative_words:\n",
    "#                         num_embedding_neg += 1\n",
    "\n",
    "#                     if w[0] in profanity:\n",
    "#                         num_profanity += 1\n",
    "#             except:\n",
    "#                 pass\n",
    "\n",
    "#     difference = num_positive - num_negative\n",
    "#     embed_difference = num_embedding_pos - num_embedding_neg\n",
    "\n",
    "#     return [difference, num_profanity, embed_difference]\n",
    "\n",
    "# vocabulary = fixed_train + fixed_test\n",
    "\n",
    "# word_embeddings_twitter100 = api.load('glove-twitter-100')\n",
    "# train_features_twitter100 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_train:\n",
    "#     train_features_twitter100.append(featurize100_twitter(i, word_embeddings_twitter100))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# test_features_twitter100 = []\n",
    "\n",
    "# count = 0\n",
    "# for i in fixed_test:\n",
    "#     test_features_twitter100.append(featurize100_twitter(i, word_embeddings_twitter100))\n",
    "    \n",
    "#     if count % 100 == 0:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "    \n",
    "# with open('./train_features_twitter100.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_features_twitter100, f)\n",
    "#     f.close()\n",
    "    \n",
    "# with open('./test_features_twitter100.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_features_twitter100, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c4d10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Features:\n",
    "\n",
    "with open('train_features25.pkl', 'rb') as f:\n",
    "    train_features_25 = pickle.load(f)\n",
    "with open('test_features25.pkl', 'rb') as f:\n",
    "    test_features_25 = pickle.load(f)\n",
    "\n",
    "\n",
    "with open('train_features50.pkl', 'rb') as f:\n",
    "    train_features_50 = pickle.load(f)\n",
    "with open('test_features50.pkl', 'rb') as f:\n",
    "    test_features_50 = pickle.load(f)\n",
    "    \n",
    "with open('train_features100.pkl', 'rb') as f:\n",
    "    train_features_100 = pickle.load(f)\n",
    "with open('test_features100.pkl', 'rb') as f:\n",
    "    test_features_100 = pickle.load(f)\n",
    "    \n",
    "with open('train_features200.pkl', 'rb') as f:\n",
    "    train_features_200 = pickle.load(f)\n",
    "with open('test_features200.pkl', 'rb') as f:\n",
    "    test_features_200 = pickle.load(f)\n",
    "    \n",
    "with open('train_features_twitter200.pkl', 'rb') as f:\n",
    "    train_features_twitter200 = pickle.load(f)\n",
    "with open('test_features_twitter200.pkl', 'rb') as f:\n",
    "    test_features_twitter200 = pickle.load(f)\n",
    "    \n",
    "with open('train_features_twitter100.pkl', 'rb') as f:\n",
    "    train_features_twitter100 = pickle.load(f)\n",
    "with open('test_features_twitter100.pkl', 'rb') as f:\n",
    "    test_features_twitter100 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666b2b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Trained on 25 embedding size train score is:  0.5361\n",
      "Logistic Regression Model Trained on 50 embedding size train score is:  0.54565\n",
      "Logistic Regression Model Trained on 100 embedding size train score is:  0.567925\n",
      "Logistic Regression Model Trained on 200 embedding size train score is:  0.547975\n",
      "Logistic Regression Model Trained on 100 twitter embedding size train score is:  0.45395\n",
      "Logistic Regression Model Trained on 200 twitter embedding size train score is:  0.462775 \n",
      "\n",
      "Logistic Regression Model Trained on 25 embedding size test score is:  0.5395\n",
      "Logistic Regression Model Trained on 50 embedding size test score is:  0.5486\n",
      "Logistic Regression Model Trained on 100 embedding size test score is:  0.5682\n",
      "Logistic Regression Model Trained on 200 embedding size test score is:  0.5496\n",
      "Logistic Regression Model Trained on 100 twitter embedding size test score is:  0.4547\n",
      "Logistic Regression Model Trained on 200 twitter embedding size test score is:  0.4675\n"
     ]
    }
   ],
   "source": [
    "# Making the model\n",
    "\n",
    "model25 = LogisticRegression()\n",
    "model50 = LogisticRegression()\n",
    "model100 = LogisticRegression()\n",
    "model200 = LogisticRegression()\n",
    "model_twitter200 = LogisticRegression()\n",
    "model_twitter100 = LogisticRegression()\n",
    "\n",
    "# Training the models\n",
    "\n",
    "model25.fit(train_features_25, token_list_train_label)\n",
    "model50.fit(train_features_50, token_list_train_label)\n",
    "model100.fit(train_features_100, token_list_train_label)\n",
    "model200.fit(train_features_200, token_list_train_label)\n",
    "model_twitter200.fit(train_features_twitter200, token_list_train_label)\n",
    "model_twitter100.fit(train_features_twitter100, token_list_train_label)\n",
    "\n",
    "# Getting the predictions\n",
    "model25_predictions = model25.predict(test_features_25)\n",
    "model50_predictions = model50.predict(test_features_50)\n",
    "model100_predictions = model100.predict(test_features_100)\n",
    "model200_predictions = model200.predict(test_features_200)\n",
    "model_twitter_200_predictions = model_twitter200.predict(test_features_twitter200)\n",
    "model_twitter_100_predictions = model_twitter100.predict(test_features_twitter100)\n",
    "\n",
    "# Getting the score\n",
    "model25_score = model25.score(test_features_25, token_list_test_label)\n",
    "model50_score = model50.score(test_features_50, token_list_test_label)\n",
    "model100_score = model100.score(test_features_100, token_list_test_label)\n",
    "model200_score = model200.score(test_features_200, token_list_test_label)\n",
    "model_twitter_200_test_score = model_twitter200.score(test_features_twitter200, token_list_test_label)\n",
    "model_twitter_100_test_score = model_twitter100.score(test_features_twitter100, token_list_test_label)\n",
    "\n",
    "model25_train_score = model25.score(train_features_25, token_list_train_label)\n",
    "model50_train_score = model50.score(train_features_50, token_list_train_label)\n",
    "model100_train_score = model100.score(train_features_100, token_list_train_label)\n",
    "model200_train_score = model200.score(train_features_200, token_list_train_label)\n",
    "model_twitter_200_train_score = model_twitter200.score(train_features_twitter200, token_list_train_label)\n",
    "model_twitter_100_train_score = model_twitter100.score(train_features_twitter100, token_list_train_label)\n",
    "\n",
    "print(\"Logistic Regression Model Trained on 25 embedding size train score is: \", model25_train_score)\n",
    "print(\"Logistic Regression Model Trained on 50 embedding size train score is: \", model50_train_score)\n",
    "print(\"Logistic Regression Model Trained on 100 embedding size train score is: \", model100_train_score)\n",
    "print(\"Logistic Regression Model Trained on 200 embedding size train score is: \", model200_train_score)\n",
    "print(\"Logistic Regression Model Trained on 100 twitter embedding size train score is: \", model_twitter_100_train_score)\n",
    "print(\"Logistic Regression Model Trained on 200 twitter embedding size train score is: \", model_twitter_200_train_score, '\\n')\n",
    "\n",
    "print(\"Logistic Regression Model Trained on 25 embedding size test score is: \", model25_score)\n",
    "print(\"Logistic Regression Model Trained on 50 embedding size test score is: \", model50_score)\n",
    "print(\"Logistic Regression Model Trained on 100 embedding size test score is: \", model100_score)\n",
    "print(\"Logistic Regression Model Trained on 200 embedding size test score is: \", model200_score)\n",
    "print(\"Logistic Regression Model Trained on 100 twitter embedding size test score is: \", model_twitter_100_test_score)\n",
    "print(\"Logistic Regression Model Trained on 200 twitter embedding size test score is: \", model_twitter_200_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00745dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision for Model of embedding size 25:  0.6904536862003781\n",
      "Positive precision for Model of embedding size 50:  0.6902173913043478\n",
      "Positive precision for Model of embedding size 100:  0.6143667296786389\n",
      "Positive precision for Model of embedding size 200:  0.6930529300567108\n",
      "Positive precision for Model of twitter embedding size 100:  0.7814272211720227\n",
      "Positive precision for Model of twitter embedding size 200:  0.8213610586011342\n"
     ]
    }
   ],
   "source": [
    "precision_25 = 0\n",
    "total_25 = 0\n",
    "\n",
    "precision_50 = 0\n",
    "total_50 = 0\n",
    "\n",
    "precision_100 = 0\n",
    "total_100 = 0\n",
    "\n",
    "precision_200 = 0\n",
    "total_200 = 0\n",
    "\n",
    "precision_twitter_100 = 0\n",
    "total_twitter_100 = 0\n",
    "\n",
    "precision_twitter_200 = 0\n",
    "total_twitter_200 = 0\n",
    "\n",
    "for i in range(len(model25_predictions)):\n",
    "    correct = token_list_test_label[i]\n",
    "    \n",
    "    if correct == 1 and model25_predictions[i] == correct:\n",
    "        precision_25 += 1\n",
    "        total_25 += 1\n",
    "    elif correct == 1:\n",
    "        total_25 += 1\n",
    "        \n",
    "    if correct == 1 and model50_predictions[i] == correct:\n",
    "        precision_50 += 1\n",
    "        total_50 += 1\n",
    "    elif correct == 1:\n",
    "        total_50 += 1\n",
    "        \n",
    "    if correct == 1 and model100_predictions[i] == correct:\n",
    "        precision_100 += 1\n",
    "        total_100 += 1\n",
    "    elif correct == 1:\n",
    "        total_100 += 1\n",
    "        \n",
    "    if correct == 1 and model200_predictions[i] == correct:\n",
    "        precision_200 += 1\n",
    "        total_200 += 1\n",
    "    elif correct == 1:\n",
    "        total_200 += 1\n",
    "        \n",
    "    if correct == 1 and model_twitter_100_predictions[i] == correct:\n",
    "        precision_twitter_100 += 1\n",
    "        total_twitter_100 += 1\n",
    "    elif correct == 1:\n",
    "        total_twitter_100 += 1\n",
    "        \n",
    "    if correct == 1 and model_twitter_200_predictions[i] == correct:\n",
    "        precision_twitter_200 += 1\n",
    "        total_twitter_200 += 1\n",
    "    elif correct == 1:\n",
    "        total_twitter_200 += 1\n",
    "        \n",
    "print(\"Positive precision for Model of embedding size 25: \", precision_25 / total_25)\n",
    "print(\"Positive precision for Model of embedding size 50: \", precision_50 / total_50)\n",
    "print(\"Positive precision for Model of embedding size 100: \", precision_100 / total_100)\n",
    "print(\"Positive precision for Model of embedding size 200: \", precision_200 / total_200)\n",
    "print(\"Positive precision for Model of twitter embedding size 100: \", precision_twitter_100 / total_twitter_100)\n",
    "print(\"Positive precision for Model of twitter embedding size 200: \", precision_twitter_200 / total_twitter_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f33f4616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative precision for Model of embedding size 25:  0.26646706586826346\n",
      "Negative precision for Model of embedding size 50:  0.26390076988879385\n",
      "Negative precision for Model of embedding size 100:  0.24807527801539778\n",
      "Negative precision for Model of embedding size 200:  0.2536355859709153\n",
      "Negative precision for Model of twitter embedding size 100:  0.26646706586826346\n",
      "Negative precision for Model of twitter embedding size 200:  0.2797262617621899\n"
     ]
    }
   ],
   "source": [
    "precision_25 = 0\n",
    "total_25 = 0\n",
    "\n",
    "precision_50 = 0\n",
    "total_50 = 0\n",
    "\n",
    "precision_100 = 0\n",
    "total_100 = 0\n",
    "\n",
    "precision_200 = 0\n",
    "total_200 = 0\n",
    "\n",
    "precision_twitter_100 = 0\n",
    "total_twitter_100 = 0\n",
    "\n",
    "precision_twitter_200 = 0\n",
    "total_twitter_200 = 0\n",
    "\n",
    "for i in range(len(model25_predictions)):\n",
    "    correct = token_list_test_label[i]\n",
    "    \n",
    "    if correct == -1 and model25_predictions[i] == correct:\n",
    "        precision_25 += 1\n",
    "        total_25 += 1\n",
    "    elif correct == -1:\n",
    "        total_25 += 1\n",
    "        \n",
    "    if correct == -1 and model50_predictions[i] == correct:\n",
    "        precision_50 += 1\n",
    "        total_50 += 1\n",
    "    elif correct == -1:\n",
    "        total_50 += 1\n",
    "        \n",
    "    if correct == -1 and model100_predictions[i] == correct:\n",
    "        precision_100 += 1\n",
    "        total_100 += 1\n",
    "    elif correct == -1:\n",
    "        total_100 += 1\n",
    "        \n",
    "    if correct == -1 and model200_predictions[i] == correct:\n",
    "        precision_200 += 1\n",
    "        total_200 += 1\n",
    "    elif correct == -1:\n",
    "        total_200 += 1\n",
    "        \n",
    "    if correct == -1 and model_twitter_100_predictions[i] == correct:\n",
    "        precision_twitter_100 += 1\n",
    "        total_twitter_100 += 1\n",
    "    elif correct == -1:\n",
    "        total_twitter_100 += 1\n",
    "        \n",
    "    if correct == -1 and model_twitter_200_predictions[i] == correct:\n",
    "        precision_twitter_200 += 1\n",
    "        total_twitter_200 += 1\n",
    "    elif correct == -1:\n",
    "        total_twitter_200 += 1\n",
    "        \n",
    "print(\"Negative precision for Model of embedding size 25: \", precision_25 / total_25)\n",
    "print(\"Negative precision for Model of embedding size 50: \", precision_50 / total_50)\n",
    "print(\"Negative precision for Model of embedding size 100: \", precision_100 / total_100)\n",
    "print(\"Negative precision for Model of embedding size 200: \", precision_200 / total_200)\n",
    "print(\"Negative precision for Model of twitter embedding size 100: \", precision_twitter_100 / total_twitter_100)\n",
    "print(\"Negative precision for Model of twitter embedding size 200: \", precision_twitter_200 / total_twitter_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4acaa9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral precision for Model of embedding size 25:  0.5393586005830904\n",
      "Neutral precision for Model of embedding size 50:  0.567930029154519\n",
      "Neutral precision for Model of embedding size 100:  0.7294460641399417\n",
      "Neutral precision for Model of embedding size 200:  0.5743440233236151\n",
      "Neutral precision for Model of twitter embedding size 100:  0.17988338192419825\n",
      "Neutral precision for Model of twitter embedding size 200:  0.1588921282798834\n"
     ]
    }
   ],
   "source": [
    "precision_25 = 0\n",
    "total_25 = 0\n",
    "\n",
    "precision_50 = 0\n",
    "total_50 = 0\n",
    "\n",
    "precision_100 = 0\n",
    "total_100 = 0\n",
    "\n",
    "precision_200 = 0\n",
    "total_200 = 0\n",
    "\n",
    "precision_twitter_100 = 0\n",
    "total_twitter_100 = 0\n",
    "\n",
    "precision_twitter_200 = 0\n",
    "total_twitter_200 = 0\n",
    "\n",
    "for i in range(len(model25_predictions)):\n",
    "    correct = token_list_test_label[i]\n",
    "    \n",
    "    if correct == 0 and model25_predictions[i] == correct:\n",
    "        precision_25 += 1\n",
    "        total_25 += 1\n",
    "    elif correct == 0:\n",
    "        total_25 += 1\n",
    "        \n",
    "    if correct == 0 and model50_predictions[i] == correct:\n",
    "        precision_50 += 1\n",
    "        total_50 += 1\n",
    "    elif correct == 0:\n",
    "        total_50 += 1\n",
    "        \n",
    "    if correct == 0 and model100_predictions[i] == correct:\n",
    "        precision_100 += 1\n",
    "        total_100 += 1\n",
    "    elif correct == 0:\n",
    "        total_100 += 1\n",
    "        \n",
    "    if correct == 0 and model200_predictions[i] == correct:\n",
    "        precision_200 += 1\n",
    "        total_200 += 1\n",
    "    elif correct == 0:\n",
    "        total_200 += 1\n",
    "        \n",
    "    if correct == 0 and model_twitter_100_predictions[i] == correct:\n",
    "        precision_twitter_100 += 1\n",
    "        total_twitter_100 += 1\n",
    "    elif correct == 0:\n",
    "        total_twitter_100 += 1\n",
    "        \n",
    "    if correct == 0 and model_twitter_200_predictions[i] == correct:\n",
    "        precision_twitter_200 += 1\n",
    "        total_twitter_200 += 1\n",
    "    elif correct == 0:\n",
    "        total_twitter_200 += 1\n",
    "        \n",
    "print(\"Neutral precision for Model of embedding size 25: \", precision_25 / total_25)\n",
    "print(\"Neutral precision for Model of embedding size 50: \", precision_50 / total_50)\n",
    "print(\"Neutral precision for Model of embedding size 100: \", precision_100 / total_100)\n",
    "print(\"Neutral precision for Model of embedding size 200: \", precision_200 / total_200)\n",
    "print(\"Neutral precision for Model of twitter embedding size 100: \", precision_twitter_100 / total_twitter_100)\n",
    "print(\"Neutral precision for Model of twitter embedding size 200: \", precision_twitter_200 / total_twitter_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0049ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
